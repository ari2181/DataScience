{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score \n",
    "from sklearn import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(r'C:\\DataScience\\PROJECT_AFIMILK\\Data\\Flat_File_for_cleansing\\df_train_80.p')\n",
    "df_val = pd.read_pickle(r'C:\\DataScience\\PROJECT_AFIMILK\\Data\\Flat_File_for_cleansing\\df_validation_80.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['FarmCode_32','split'])\n",
    "df_val = df_val.drop(columns=['FarmCode_32','split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2140, 78)\n",
      "(2140,)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train.drop(columns= 'CurMet_t7_t30')\n",
    "y_train = df_train['CurMet_t7_t30'].astype('category')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(536, 78)\n",
      "(536,)\n"
     ]
    }
   ],
   "source": [
    "X_val = df_val.drop(columns= 'CurMet_t7_t30')\n",
    "y_val = df_val['CurMet_t7_t30'].astype('category')\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalized the variables\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "##\n",
    "scaler_val = preprocessing.StandardScaler().fit(X_val)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    y_pred = model.predict(test_features)\n",
    "    prf1 = precision_recall_fscore_support(test_labels,y_pred)\n",
    "    recall = prf1[1][1]\n",
    "    auc = roc_auc_score(test_labels, y_pred)\n",
    "    ##\n",
    "    print('Model Performance')\n",
    "    print('Recall: {:0.4f}'.format(recall))\n",
    "    print('AUC: {:0.4f}'.format(auc))\n",
    "    return recall, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 3, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2, 3, 4, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 3, 5, 10], 'min_samples_leaf': [2, 3, 4, 5], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   49.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 23.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 36.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 49.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 65.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 84.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 104.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 126.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 151.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 178.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 208.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 234.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 255.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestClassifier(), n_iter=1000,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [2, 3, 4, 5],\n",
       "                                        'min_samples_split': [2, 3, 5, 10],\n",
       "                                        'n_estimators': [100, 200, 300, 400,\n",
       "                                                         500, 600, 700, 800,\n",
       "                                                         900, 1000]},\n",
       "                   random_state=42, scoring='roc_auc', verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator= rf, param_distributions= random_grid, n_iter= 1000, cv =10, \n",
    "                               verbose= 2, random_state= 42, n_jobs= -1, scoring= 'roc_auc' )\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 3,\n",
       " 'min_samples_leaf': 5,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 10,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Recall: 0.7312\n",
      "AUC: 0.8616\n"
     ]
    }
   ],
   "source": [
    "base_model_rf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\n",
    "base_model_rf.fit(X_train_scaled, y_train)\n",
    "base_accuracy_rf = evaluate(base_model_rf, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  fine-tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Recall: 0.7250\n",
      "AUC: 0.8585\n"
     ]
    }
   ],
   "source": [
    "best_random_rf = rf_random.best_estimator_\n",
    "random_accuracy_rf = evaluate(best_random_rf, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "filename = r'C:\\DataScience\\PROJECT_AFIMILK\\Data\\Flat_File_for_cleansing\\rf_final_model.p'\n",
    "pickle.dump(base_model_rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = [\"gini\", \"entropy\"]\n",
    "#splitter = [\"best\", \"random\"]\n",
    "max_depth = [int(x) for x in np.linspace(start = 2, stop = 10, num = 9)]\n",
    "min_samples_split = [2,3,4,5]\n",
    "min_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 10, num = 9)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 9)]\n",
    "\n",
    "ada_random_grid = {\"base_estimator__criterion\" : criterion,\n",
    "                  #\"base_estimator__splitter\" : splitter  ,\n",
    "                  \"base_estimator__max_depth\" : max_depth,\n",
    "                   \"base_estimator__min_samples_split\" : min_samples_split,\n",
    "                   \"base_estimator__min_samples_leaf\" : min_samples_leaf,\n",
    "                   #\"base_estimator__max_features\" : max_features,\n",
    "                  \"n_estimators\": n_estimators\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "ABC = AdaBoostClassifier(base_estimator = DTC)\n",
    "\n",
    "random_search_ABC = RandomizedSearchCV(estimator= ABC, param_distributions= ada_random_grid, n_iter= 1000, cv =10, \n",
    "                               verbose= 2, random_state= 42, n_jobs= -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 30.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 40.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 140.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 151.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 165.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 179.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 196.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 211.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 229.2min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 247.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 263.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10,\n",
       "                   estimator=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42)),\n",
       "                   n_iter=1000, n_jobs=-1,\n",
       "                   param_distributions={'base_estimator__criterion': ['gini',\n",
       "                                                                      'entropy'],\n",
       "                                        'base_estimator__max_depth': [2, 3, 4,\n",
       "                                                                      5, 6, 7,\n",
       "                                                                      8, 9,\n",
       "                                                                      10],\n",
       "                                        'base_estimator__min_samples_leaf': [2,\n",
       "                                                                             3,\n",
       "                                                                             4,\n",
       "                                                                             5,\n",
       "                                                                             6,\n",
       "                                                                             7,\n",
       "                                                                             8,\n",
       "                                                                             9,\n",
       "                                                                             10],\n",
       "                                        'base_estimator__min_samples_split': [2,\n",
       "                                                                              3,\n",
       "                                                                              4,\n",
       "                                                                              5],\n",
       "                                        'n_estimators': [10, 21, 32, 43, 55, 66,\n",
       "                                                         77, 88, 100]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_ABC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 43,\n",
       " 'base_estimator__min_samples_split': 4,\n",
       " 'base_estimator__min_samples_leaf': 3,\n",
       " 'base_estimator__max_depth': 8,\n",
       " 'base_estimator__criterion': 'entropy'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search_ABC.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Recall: 0.7750\n",
      "AUC: 0.8769\n"
     ]
    }
   ],
   "source": [
    "base_model_ada = AdaBoostClassifier(random_state=1)\n",
    "base_model_ada.fit(X_train_scaled, y_train)\n",
    "base_accuracy_ada = evaluate(base_model_ada, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fine-tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Recall: 0.7000\n",
      "AUC: 0.8487\n"
     ]
    }
   ],
   "source": [
    "best_random_ada = random_search_ABC.best_estimator_\n",
    "random_accuracy_ada = evaluate(best_random_ada, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "filename = r'C:\\DataScience\\PROJECT_AFIMILK\\Data\\Flat_File_for_cleansing\\adaboost_final_model.p'\n",
    "pickle.dump(base_model_ada, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tolerance for stopping criteria. float, default=1e-4\n",
    "tol = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3]\n",
    "# Algorithm to use in the optimization problem.\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "# Maximum number of iterations taken for the solvers to converge.  int, default=100\n",
    "max_iter = [int(x) for x in np.linspace(start = 100, stop = 10000, num = 100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tol': [1e-05, 5e-05, 0.0001, 0.0005, 0.001, 0.005], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'max_iter': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, 3100, 3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100, 4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100, 5200, 5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200, 6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200, 7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200, 8300, 8400, 8500, 8600, 8700, 8800, 8900, 9000, 9100, 9200, 9300, 9400, 9500, 9600, 9700, 9800, 9900, 10000]}\n"
     ]
    }
   ],
   "source": [
    "# Create the random grid\n",
    "lr_random_grid = {'tol': tol,\n",
    "               'solver': solver,\n",
    "               'max_iter': max_iter\n",
    "               }\n",
    "\n",
    "print(lr_random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 25.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 34.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 45.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 62.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 80.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 96.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 116.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 140.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 164.1min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 189.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed: 220.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=LogisticRegression(), n_iter=1000,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_iter': [100, 200, 300, 400, 500,\n",
       "                                                     600, 700, 800, 900, 1000,\n",
       "                                                     1100, 1200, 1300, 1400,\n",
       "                                                     1500, 1600, 1700, 1800,\n",
       "                                                     1900, 2000, 2100, 2200,\n",
       "                                                     2300, 2400, 2500, 2600,\n",
       "                                                     2700, 2800, 2900, 3000, ...],\n",
       "                                        'solver': ['newton-cg', 'lbfgs',\n",
       "                                                   'liblinear', 'sag', 'saga'],\n",
       "                                        'tol': [1e-05, 5e-05, 0.0001, 0.0005,\n",
       "                                                0.001, 0.005]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "random_search_LR = RandomizedSearchCV(estimator= lr, param_distributions= lr_random_grid, n_iter= 1000, cv =10, \n",
    "                               verbose= 2, random_state= 42, n_jobs= -1)\n",
    "\n",
    "random_search_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Recall: 0.7688\n",
      "AUC: 0.8684\n"
     ]
    }
   ],
   "source": [
    "base_model_lr = LogisticRegression(max_iter= 1000, random_state= 1)\n",
    "base_model_lr.fit(X_train_scaled, y_train)\n",
    "base_accuracy_lr = evaluate(base_model_lr, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fine-tunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Recall: 0.7188\n",
      "AUC: 0.8554\n"
     ]
    }
   ],
   "source": [
    "best_random_lr = random_search_LR.best_estimator_\n",
    "random_accuracy_lr = evaluate(best_random_lr, X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "filename = r'C:\\DataScience\\PROJECT_AFIMILK\\Data\\Flat_File_for_cleansing\\lr_final_model.p'\n",
    "pickle.dump(base_model_lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
